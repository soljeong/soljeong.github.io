## overview
- https://dacon.io/competitions/official/236170/overview/description
- 속상하다 속상해
- https://drive.google.com/open?id=18nMRmqrNOUOKnSH52T67nv08dffZGCFf&usp=drive_fs
## 숙제
- torch.cat()부분 어떻게 됐는지 디버깅 필요
- 어떤 피처가 문제에 설명력을 가지는지 판단할 수 있는 지표?
### 노트북
- https://velog.io/@cheol2_y/%EC%82%AC%EC%9A%A9%EB%B2%95
- 구글 드라이브의 폴더를 공유
- 공유 받은 사람이 본인 드라이브 루트에 바로가기 생성
- 노트북을 공유해도 각 사용자가 각자의 런타임에서 실행하게 되고
- 구글 드라이브 연결해도 각자의 드라이브에 연결됨
- 하지만 아까 바로가기 만들었으니까 경로는 동일하게 유지할 수 있음
- 서로 다른 환경이지만 코드 바꾸지 않고 실행 가능하다.
### eda 그래프
- eda 관련된 그래프는 굳이 매번 새로 코드 실행시켜서 다시 그릴 필요가 크지 않으니까
- 어지간하면 코드는 로컬에 남겨놓고 결과물만 노트북에 포함한다
- 아니면 코드를 마크다운 변경해서 같이 올린다
## 정규화
- 데이터를 편향되게 준거라면 정규화를 잘 해야하는거 아닌지
## hyper parameter
- ng sample
- ReduceLROnPlateau : factor
- ReduceLROnPlateau : patience
- LR 초기값
- drop out
- layer num
- model factor num : gpu에 올라가는 용량 커진다, 트레이닝 시간 길어짐, 평가 시간 길어짐
- embedding dimention num
- dataloader : batch size
## 시도
### 2023-11-12
- 50에포크에 10분인데 평가에 20분 걸리는건 좀 심한데
- 평가에서 trainloader를 쓰지 않아서 시간이 오래 걸리는 것 같은데
- 트레인은 20만 건, 평가는 5500만 건이니까 오래 걸리는게 당연한가?
- 로스가 일정 크기 이상이면 평가 안하고 에포크 계속 하는 조건 넣을까?
- 100번만에  0.08
- 지금 사용하고 있는 피처의 효과 평가 어려움
- 팩터 늘린것이 점수에 어떤 효과 있을지
- 0.02까지 내려간 다음에 잘 안내려가네 -> full data recall 2% 밖에 안됨
- lr 0.001로 바꿔서 더 돌려본다 -> 로스 내려감

### 2023-11-10
- 시피유에서 돌아가는지? -> 노트북에서 안돌아감, 반응 없음
- 러닝레이트 : 500회인가 돌렸을 때 로스 1%대 목표로
	- 첫번째 시도 : 페이션트 50, 팩터 0.5, 초기값 0.1 : 초기값 0.1 나쁜 선택인듯 0
	- 페이션트 50, 팩터 0.5, 초기값 0.02 : 초기값을 더 올려야할듯, 페이션트 작동 안하고 100회 넘게 로스 내려감
		- Epoch 156/1000, Loss: 0.096708, LR: 0.02 한번도 안바꾸고 여기까지 내려옴
		- 어디까지 가나 보자
		- Epoch 241/1000, Loss: 0.063073, LR: 0.02 한번도 안바꾸고 여기까지 내려옴 46분
- 이벨류에이트 안할 수 없다 결국, 백회마다 한번씩은 해야할 듯, 오버핏을 피하려면
	- 목표 로스를 얼마로 잡아야 할지?
	- 이걸 알려면 이벨류에이트 해야한다. 로스 떨어지는거에 따라 리콜 점수 달라지는지 확인 필요
	- 목표 로스가 정해지면, 거기에 빨리 도달하는 러닝레이트를 신경써야함
	- 그러고 나면 피처 바꾸는 실험 가능
	- 근데 피처 바꾸고 나면 또 목표 로스가 바뀌는거 아닌가?
- 네거티브샘플링 : 에포크 100번마다 네거티브 샘플링 새로하면 : 샘플링 할때마다 로스가 다시 올라갈텐데, 반복하면 할수록 로스가 떨어져야한다. 과연 그럴까?
- 
- 오버핏 여부 확인하려면, 트레인-테스트 스플릿 해야하는지?
	- 아니 근데 오버핏 되기 전에 이미 점수가 너무 낮은데 이건 어쩔
- gpu 없으면 점수계산한번하는데만 5분 걸림
- 유료 결제해야하나
- degree-education 관계
![[Pasted image 20231110121733.png]]
- 모델 변경
	- 학습 안됨
- 모델을 간소화해서 트라이
	- Epoch 547/1000, Loss: 0.124676, LR: 1.9073486328125e-08
Epoch 548/1000, Loss: 0.124931, LR: 1.9073486328125e-08
Epoch 549/1000, Loss: 0.142291, LR: 1.9073486328125e-08
recall train : 0.03807082027196884, recall test : 0.009793287143111229
Epoch 550/1000, Loss: 0.129944, LR: 1.9073486328125e-08
Epoch 551/1000, Loss: 0.137621, LR: 1.9073486328125e-08
Epoch 552/1000, Loss: 0.137720, LR: 1.9073486328125e-08
Epoch 553/1000, Loss: 0.138952, LR: 1.9073486328125e-08

로스가 떨어지지 않음
- 모델 펙터를 늘리면 평가가 오래걸림 1000->500-> 50 시도중
- 모델 펙터 늘리면 gpu 메모리도 많이 차지함
- 배치사이즈 제약조건
- recall train : 0.022370902821421623, recall test : 0.004961486905813217 Epoch 50/1000, Loss: 0.178440, LR: 0.01 Epoch 51/1000, Loss: 0.242314, LR: 0.01
- 러닝레이트 조정할꺼면, 네거티브 샘플링 바꾸지 말아야함

## training
- 텐서보드 사용
- 시간계산 사용
- [ ] epoch마다 loss 변하는거 그래프
- [x] 어제 추가한 부분 문단 정리
- [ ] eda를 위해. 행렬곱 사용?
## 편의성
- [x] 노트북 실행 환경이 코랩인지 아닌지 확인하는 방법 있나?
```python
try:
    from google.colab import drive

    drive.mount('/content/drive')
    dir_path = '/content/drive/MyDrive/dacon/csv/'
    print("노트북은 Google Colab에서 실행 중입니다.")

except:
    dir_path = './csv/'
    print("노트북은 Google Colab이 아닌 환경에서 실행 중입니다.")

```
## evaluate
- train data case인데, model이 6위이하로 판단할 수도 있잖아
- 리콜 함수 다시 읽어보는거
- 스플릿 한 다음에, 에포크 한번 끝날때마다 테스트 셋 평가
- 여기서의 평가는 0/1로 맞추면 되는거니까. 이게 높은걸로 하면 되겠네
- 평가를 진짜 만들긴 만들어야하는데
	- split 해놓고, test를 얼마나 잘 맞추는지 recall로 측정
	- split 하지 않고, 전체 매트릭스로 recall 측정
### split
- 매트릭스 모양을 유지하면서 둘로 나눈다.
```python
import numpy as np

def train_test_split(user_item_matrix, test_size=0.2, seed=None):
    """
    user_item_matrix: 사용자-아이템 상호작용이 기록된 NumPy 행렬
    test_size: 각 사용자에 대해 테스트 세트로 설정할 상호작용의 비율
    seed: 난수 시드값 (재현가능한 결과를 위해)

    반환값: train_matrix, test_matrix
    """
    np.random.seed(seed)  # 재현가능성을 위한 시드 설정
    train_matrix = user_item_matrix.copy()
    test_matrix = np.zeros(user_item_matrix.shape)

    for user in range(user_item_matrix.shape[0]):
        positive_interactions = np.where(user_item_matrix[user, :] > 0)[0]
        test_interactions = np.random.choice(positive_interactions, 
                                             size=int(len(positive_interactions) * test_size), 
                                             replace=False)

        # 선택된 상호작용을 테스트 세트로 설정하고 훈련 세트에서 제거
        train_matrix[user, test_interactions] = 0
        test_matrix[user, test_interactions] = user_item_matrix[user, test_interactions]
    
    # True/False 행렬로 변환된 상태에서 반환하거나 0과 1로 변환할 수 있습니다.
    return train_matrix, test_matrix

# 예시 사용자-아이템 행렬
user_item_matrix = np.random.randint(0, 5, size=(100, 200))

# 훈련 세트와 테스트 세트로 분리
train_matrix, test_matrix = train_test_split(user_item_matrix, test_size=0.2, seed=42)

print(f"Train matrix shape: {train_matrix.shape}")
print(f"Test matrix shape: {test_matrix.shape}")

```

### train-test split 성철님
```python
#학습, 검증 분리
# 이코드 그대로 복사해서 (주석빼고) gpt에게 이 코드 해석해줘 하면 자세히 알려줍니다!
train, test = [], []
apply_train_groupby = apply_train.groupby('resume_seq')['recruitment_seq'].apply(list)
# uid에는 resume가 , iid에는 recruiment가 들어간다.
for uid, iids in zip(apply_train_groupby.index.tolist(), apply_train_groupby.values.tolist()):
    # 보시는 대로 iid의 마지막 원소를 제외하고 train에 [resume, recruimet]로 append 된다.
    for iid in iids[:-1]:
        train.append([uid,iid])
    # 나머지 하나는 test에 append 시킴
    test.append([uid, iids[-1]])
```
- 모든 유저는 최소 2건 이상 케이스 있음
- 각각 유저별로 마지막 한건의 아이템 선택을 테스트로 빼놓는다.
- 치우치면 어떡하지?

### recall
```python
import torch

def predict_all_scores_and_recall(model, user_features, item_features, user_item_matrix, k=10):
    user_features = torch.from_numpy(user_features).to(device)
    item_features = torch.from_numpy(item_features).to(device)
    user_item_matrix = torch.from_numpy(user_item_matrix).to(device)

    model.eval()  # Set the model to evaluation mode
    user_num, item_num = user_item_matrix.shape
    all_scores = torch.zeros(user_num, item_num).to(device)
    recall_scores = torch.zeros(user_num).to(device)

    with torch.no_grad():
        for user_id in range(user_num):
            # Repeat user features for all items
            user_feature_repeated = user_features[user_id].repeat(item_num, 1).to(device)
            user_id_tensor = torch.tensor([user_id] * item_num).to(device)

            # Get predictions
            scores = model(user_id_tensor, user_feature_repeated, torch.arange(item_num).to(device), item_features).squeeze()
            all_scores[user_id] = scores

            # Calculate recall
            _, top_indices = torch.topk(scores, k)
            true_items = user_item_matrix[user_id] == 1
            num_relevant_items = true_items.sum()
            num_relevant_and_recommended = true_items[top_indices].sum()
            recall_scores[user_id] = num_relevant_and_recommended.float() / num_relevant_items.float() if num_relevant_items > 0 else torch.tensor(0.0).to(device)

    return all_scores, recall_scores

```

## dataset, dataloader
- [ ] breakpoint 쓸 줄 몰라
- 지금 계속 노트북이 늘어나기만하고, 뭐가 진행되고 있는지 전혀 감이 안오네
- dataloader는 dataset에 정의된 get_item을 통해 데이터를 가져온다
- 길이를 가지고 있고
- 한개씩 꺼내온다, 일정 길이로 잘라서 모델에 넣어주는것
- user-item matrix는 dataset에 들어있고, 모델에 넘겨주는 것이 아님
- apply_train 길이가 positive feature len
- negative sampling을 해서 len이 늘어난다
- dataset에서는 무조건 한번에 하나의 케이스 씩 출력한다고 생각하고 만들어야함
## 모델

### cosine_similarity
- user-item matrix를 넣고
- user-user similarity를 얻는다
- (8,6)을 넣고 (8,8) 리턴
```python
# 사용자 간의 유사성 계산
user_similarity = cosine_similarity(user_item_matrix)

# 추천 점수 계산
user_predicted_scores = user_similarity.dot(user_item_matrix) / np.array([np.abs(user_similarity).sum(axis=1)]).T
```
- 한명의 유저 입장에서는
- 나 외의 모든 유저들과의 유사성 계산
- 내 기준 합계로 나눠서 비율로 만듦, 0~1 사이 값
- user-item matrix에 곱한다. 0, 1,로 구성되어 있으니까, 지원한 유저들의 정보만 남는다.
- 내가 내적의 의미를 아직 잘 모르고 있는것
- 가중치, 내 특성 스칼라에다가. 나와 비슷한 유저들의 특성을 가중한다. 비슷할수록 많이 더한다.
- [[Numpy 파이썬 내적, 행렬곱 함수 np.dot() 사용법 총정리]]

- 이거는 다차원으로 할 수 없나?

### NCF
- https://doheelab.github.io/recommender-system/ncf_mf/
- 이 블로그 아저씨 코드대로 일단 실행 되는지
- 실행되면 코랩에 올려본다 일단
- https://d2l.ai/chapter_recommender-systems/index.html
- [[[21. Recommender Systems — Dive into Deep Learning 1.0.3 documentation]]]

## table
![[국민대 table]]

